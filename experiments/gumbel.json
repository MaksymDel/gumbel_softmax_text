{
    "vocabulary": {
      "max_vocab_size": 32000
    },
    "dataset_reader":{
      "type":"simple_seq2seq",
      "source_token_indexers":{"tokens": {"namespace": "source_words", "lowercase_tokens": true}},
      "target_token_indexers":{"tokens": {"namespace": "target_words", "lowercase_tokens": true}}
    },
    "train_data_path": "data/de-en/train.tok.do50.both",
    "validation_data_path": "data/de-en/valid.tok.do50.both",
    "model": {
       "type": "vanilla_softmax_gumbel_detach",

      "scheduled_sampling_ratio": 0.5, // 0 means complete teacher forcing

      "weight_function": "gumbel",

      // has an effect only if "weight_function" is "gumbel"
      "gumbel_tau": 1,
      "gumbel_hard": true,
      "gumbel_eps": 1e-10,

      "infer_with_argmax": true,
      "gumbel_argmax_from_logits": false,

      "detach_self_feeding": true,

      "source_embedder": {
        "tokens": {
          "type": "embedding",
          "vocab_namespace": "source_words",
          "embedding_dim": 300,
          "trainable": true
        }
      },
      "encoder": {
        "type": "lstm",
        "input_size": 300,
        "hidden_size": 150,
        "num_layers": 1,
        "bidirectional": true
      },
      "max_decoding_steps": 50,
      "target_namespace": "target_words",
      "attention_function": {"type": "dot_product"}
    },
    "iterator": {
      "type": "basic",
      "batch_size": 128
    },
    "trainer": {
      "num_epochs": 500,
      "patience": 10,
      "cuda_device": 0,
      "optimizer": {
        "type": "adam",
        "lr": 0.001,
        "betas": [0.9, 0.9]
      }
    }
  }
