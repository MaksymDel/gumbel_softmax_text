{
    "vocabulary": {
      "max_vocab_size": 50000
    },
    "dataset_reader":{
      "type":"simple_seq2seq",
      "source_token_indexers":{"tokens": {"namespace": "source_words", "lowercase_tokens": true}},
      "target_token_indexers":{"tokens": {"namespace": "target_words", "lowercase_tokens": true}}
    },
    "train_data_path": "data/de-en/train.tok.do50.both",
    #"train_data_path": "data/de-en/valid.tok.do50.both", # for debug
    "validation_data_path": "data/de-en/valid.tok.do50.both",
    "model": {
      "type": "regr_seq2seq",
      "scheduled_sampling_ratio": 0.0,
      "loss_type": "mse",
      "source_embedder": {
        "tokens": {
          "type": "embedding",
          "vocab_namespace": "source_words",
          "embedding_dim": 300,
          "trainable": true
        }
      },
      "encoder": {
        "type": "lstm",
        "input_size": 300,
        "hidden_size": 500,
        "num_layers": 2,
        "bidirectional": true
      },
      "max_decoding_steps": 50,
      "target_namespace": "target_words",
      "attention_function": {"type": "dot_product"},
      "target_embedding_dim": 300,
      "target_tokens_embedder": {
          "type": "embedding",
          "vocab_namespace": "target_words",
          "embedding_dim": 300,
          "pretrained_file": "data/cc.en.300.vec.gz",
          "trainable": false
      },
      "output_projection_layer": {
         "input_dim": 500+500, # must be equel to encdoer's out dim
         "num_layers": 2,
         "hidden_dims": [600, 300], # last dim must be equel to target emb dim
         "activations": ["tanh", "linear"],
         "dropout": [0.2, 0.0]

      }
    },
    "iterator": {
      "type": "basic",
      "batch_size": 256
    },
    "trainer": {
      "grad_norm": 5.0,
      "num_epochs": 40,
      "patience": 5,
      "cuda_device": 0,
      "optimizer": {
        "type": "sgd",
        "lr": 0.01
      },
      "learning_rate_scheduler": {
      "type": "exponential",
      "gamma": 0.7 
      }
    }
  }
