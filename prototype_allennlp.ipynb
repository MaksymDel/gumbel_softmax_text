{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_X = \"europarl/dev.en\"\n",
    "path_dev_Y = \"europarl/dev.et\"\n",
    "path_train_X = \"europarl/train.en\"\n",
    "path_train_Y = \"europarl/train.et\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "\n",
    "class MonolingualDatasetReader(DatasetReader):\n",
    "    def __init__(self, lazy: bool = False, max_sent_len = 50) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._sentence_tokenizer = WordTokenizer(word_splitter=JustSpacesWordSplitter())\n",
    "        self._sentence_token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self._sentence_add_start_token = True\n",
    "        self._max_sent_len = max_sent_len\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path):\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            print(\"Reading instances from lines in file at: %s\", file_path)\n",
    "            for line_num, line in enumerate(data_file):\n",
    "                line = line.strip(\"\\n\")\n",
    "\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                line = line.lower()\n",
    "                tokenized_sentence = self._sentence_tokenizer.tokenize(line)\n",
    "                if len(tokenized_sentence) > self._max_sent_len:\n",
    "                    continue\n",
    "\n",
    "                yield self.text_to_instance(tokenized_sentence)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokenized_sentence) -> Instance:  # type: ignore\n",
    "        # pylint: disable=arguments-differ\n",
    "        if self._sentence_add_start_token:\n",
    "            tokenized_sentence.insert(0, Token(START_SYMBOL))\n",
    "        tokenized_sentence.append(Token(END_SYMBOL))\n",
    "        sentence_field = TextField(tokenized_sentence, self._sentence_token_indexers)\n",
    "        \n",
    "        return Instance({'sentence': sentence_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_dataset_reader = MonolingualDatasetReader(lazy=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [00:00, 15551.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading instances from lines in file at: %s europarl/train.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9573it [00:00, 12260.01it/s]\n",
      "500it [00:00, 39692.48it/s]\n",
      "4186it [00:00, 15799.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading instances from lines in file at: %s europarl/dev.en\n",
      "Reading instances from lines in file at: %s europarl/train.et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920it [00:00, 14243.95it/s]\n",
      "500it [00:00, 41573.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading instances from lines in file at: %s europarl/dev.et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "en_instances_train = mono_dataset_reader.read(path_train_X)\n",
    "en_instances_dev = mono_dataset_reader.read(path_dev_X)\n",
    "\n",
    "et_instances_train = mono_dataset_reader.read(path_train_Y)\n",
    "et_instances_dev = mono_dataset_reader.read(path_dev_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10073/10073 [00:00<00:00, 36825.04it/s]\n",
      "100%|██████████| 10420/10420 [00:00<00:00, 42041.69it/s]\n"
     ]
    }
   ],
   "source": [
    "en_vocab = Vocabulary.from_instances(instances=en_instances_train + en_instances_dev, max_vocab_size=20000)\n",
    "et_vocab = Vocabulary.from_instances(instances=et_instances_train + et_instances_dev, max_vocab_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.iterators import BasicIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_iterator_creator = BucketIterator(sorting_keys = [(\"sentence\", \"num_tokens\")], batch_size=32, max_instances_in_memory=None)\n",
    "# et_iterator_creator = BucketIterator(sorting_keys = [(\"sentence\", \"num_tokens\")], batch_size=32, max_instances_in_memory=None)\n",
    "\n",
    "en_iterator_creator = BasicIterator(batch_size=32, max_instances_in_memory=None)\n",
    "et_iterator_creator = BasicIterator(batch_size=32, max_instances_in_memory=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_iterator_creator.index_with(en_vocab)\n",
    "et_iterator_creator.index_with(et_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_batch_iterator_train = en_iterator_creator(instances=en_instances_train, num_epochs=None, shuffle=False)\n",
    "en_batch_iterator_dev = en_iterator_creator(instances=en_instances_dev, num_epochs=None, shuffle=False)\n",
    "\n",
    "et_batch_iterator_train = et_iterator_creator(instances=et_instances_train, num_epochs=None, shuffle=False)\n",
    "et_batch_iterator_dev = et_iterator_creator(instances=et_instances_dev, num_epochs=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = et_batch_iterator_dev.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy\n",
    "from overrides import overrides\n",
    "\n",
    "import torch\n",
    "from torch.nn.modules.rnn import LSTMCell\n",
    "from torch.nn.modules.linear import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2SeqEncoder\n",
    "from allennlp.modules.attention import DotProductAttention\n",
    "from allennlp.modules.token_embedders import Embedding, TokenEmbedder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits, weighted_sum, get_final_encoder_states\n",
    "\n",
    "\n",
    "class VanillaRnn2Rnn(Model):\n",
    "    \"\"\"\n",
    "    Returns predicted indeces  \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 source_vocab: Vocabulary,\n",
    "                 target_vocab: Vocabulary,\n",
    "                 source_embedding: Embedding,\n",
    "                 target_embedding: Embedding,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 max_decoding_steps: int\n",
    "                 ) -> None:\n",
    "        \n",
    "        super(VanillaRnn2Rnn, self).__init__(source_vocab)\n",
    "        \n",
    "        self._source_vocab = source_vocab\n",
    "        self._target_vocab = target_vocab\n",
    "        \n",
    "        self._source_embedder = source_embedding\n",
    "        self._target_embedder = target_embedding\n",
    "        \n",
    "        self._encoder = encoder\n",
    "        \n",
    "        self._max_decoding_steps = max_decoding_steps\n",
    "         \n",
    "        # We need the start symbol to provide as the input at the first timestep of decoding, and\n",
    "        # end symbol as a way to indicate the end of the decoded sequence.\n",
    "        self._start_index = self._target_vocab.get_token_index(START_SYMBOL, \"tokens\")\n",
    "        self._end_index = self._target_vocab.get_token_index(END_SYMBOL, \"tokens\")\n",
    "        \n",
    "        num_classes = self._target_vocab.get_vocab_size(\"tokens\")\n",
    "        \n",
    "        # Decoder output dim needs to be the same as the encoder output dim since we initialize the\n",
    "        # hidden state of the decoder with that of the final hidden states of the encoder. Also, if\n",
    "        # we're using attention with ``DotProductSimilarity``, this is needed.\n",
    "        self._decoder_output_dim = self._encoder.get_output_dim()\n",
    "        \n",
    "        target_embedding_dim = self._target_embedder.get_output_dim()    \n",
    "            \n",
    "        self._decoder_attention = DotProductAttention()\n",
    "        # The output of attention, a weighted average over encoder outputs, will be\n",
    "        # concatenated to the input vector of the decoder at each time step.\n",
    "        self._decoder_input_dim = self._encoder.get_output_dim() + target_embedding_dim\n",
    "\n",
    "        # TODO (pradeep): Do not hardcode decoder cell type.\n",
    "        self._decoder_cell = LSTMCell(self._decoder_input_dim, self._decoder_output_dim)\n",
    "        \n",
    "        self._output_projection_layer = Linear(self._decoder_output_dim, num_classes)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                sentence: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Decoder logic for producing the entire target sequence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence : torch.LongTensor\n",
    "           Tensor of padded batch of indexed source strings\n",
    "        \"\"\"\n",
    "        # (batch_size, input_sequence_length, encoder_output_dim)\n",
    "        #print(sentence)\n",
    "        tokens_ids = sentence[\"tokens\"]\n",
    "        embedded_input = self._source_embedder(tokens_ids)\n",
    "        batch_size, _, _ = embedded_input.size()\n",
    "        source_mask = get_text_field_mask(sentence)\n",
    "        encoder_outputs = self._encoder(embedded_input, source_mask)\n",
    "        final_encoder_output = get_final_encoder_states(encoder_outputs, source_mask, True)  # (batch_size, encoder_output_dim)\n",
    "        \n",
    "        num_decoding_steps = self._max_decoding_steps\n",
    "        \n",
    "        decoder_hidden = final_encoder_output\n",
    "        \n",
    "        decoder_context = encoder_outputs.new_zeros(batch_size, self._decoder_output_dim)\n",
    "        \n",
    "        last_predictions = None\n",
    "        step_logits = []\n",
    "        step_probabilities = []\n",
    "        step_predictions = []\n",
    "        for timestep in range(num_decoding_steps):\n",
    "            if timestep == 0:\n",
    "                # For the first timestep, when we do not have targets, we input start symbols.\n",
    "                # (batch_size,)\n",
    "                input_indices = source_mask.new_full((batch_size,), fill_value=self._start_index)\n",
    "            else:\n",
    "                input_indices = last_predictions # last predictions should be differentiable\n",
    "            decoder_input = self._prepare_decode_step_input(input_indices, decoder_hidden,\n",
    "                                                            encoder_outputs, source_mask)\n",
    "            decoder_hidden, decoder_context = self._decoder_cell(decoder_input,\n",
    "                                                                 (decoder_hidden, decoder_context))\n",
    "            \n",
    "            # (batch_size, num_classes)\n",
    "            output_projections = self._output_projection_layer(decoder_hidden)\n",
    "            # list of (batch_size, 1, num_classes)\n",
    "            step_logits.append(output_projections.unsqueeze(1))\n",
    "\n",
    "            class_probabilities = F.softmax(output_projections, dim=-1)\n",
    "            step_probabilities.append(class_probabilities.unsqueeze(1))\n",
    "\n",
    "            _, predicted_classes = torch.max(class_probabilities, 1) # non differentiable sampling step\n",
    "            # (batch_size, 1)\n",
    "            step_predictions.append(predicted_classes.unsqueeze(1))\n",
    "            \n",
    "            last_predictions = predicted_classes\n",
    "\n",
    "        # step_logits is a list containing tensors of shape (batch_size, 1, num_classes)\n",
    "        # This is (batch_size, num_decoding_steps, num_classes)\n",
    "        logits = torch.cat(step_logits, 1)\n",
    "        class_probabilities = torch.cat(step_probabilities, 1)\n",
    "        all_predictions = torch.cat(step_predictions, 1)\n",
    "        \n",
    "        output_dict = {\"logits\": logits,\n",
    "                       \"class_probabilities\": class_probabilities,\n",
    "                       \"predictions\": all_predictions}\n",
    "        \n",
    "        return output_dict\n",
    "\n",
    "    def _prepare_decode_step_input(self,\n",
    "                                   input_indices: torch.LongTensor,\n",
    "                                   decoder_hidden_state: torch.LongTensor = None,\n",
    "                                   encoder_outputs: torch.LongTensor = None,\n",
    "                                   encoder_outputs_mask: torch.LongTensor = None) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Given the input indices for the current timestep of the decoder, and all the encoder\n",
    "        outputs, compute the input at the current timestep.  Note: This method is agnostic to\n",
    "        whether the indices are gold indices or the predictions made by the decoder at the last\n",
    "        timestep. So, this can be used even if we're doing some kind of scheduled sampling.\n",
    "        If we're not using attention, the output of this method is just an embedding of the input\n",
    "        indices.  If we are, the output will be a concatentation of the embedding and an attended\n",
    "        average of the encoder inputs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_indices : torch.LongTensor\n",
    "            Indices of either the gold inputs to the decoder or the predicted labels from the\n",
    "            previous timestep.\n",
    "        decoder_hidden_state : torch.LongTensor, optional (not needed if no attention)\n",
    "            Output of from the decoder at the last time step. Needed only if using attention.\n",
    "        encoder_outputs : torch.LongTensor, optional (not needed if no attention)\n",
    "            Encoder outputs from all time steps. Needed only if using attention.\n",
    "        encoder_outputs_mask : torch.LongTensor, optional (not needed if no attention)\n",
    "            Masks on encoder outputs. Needed only if using attention.\n",
    "        \"\"\"\n",
    "        # input_indices : (batch_size,)  since we are processing these one timestep at a time.\n",
    "        # (batch_size, target_embedding_dim)\n",
    "        embedded_input = self._target_embedder(input_indices) # this should be sperate func that work with different\n",
    "        # forms of in\n",
    "        \n",
    "        # encoder_outputs : (batch_size, input_sequence_length, encoder_output_dim)\n",
    "        # Ensuring mask is also a FloatTensor. Or else the multiplication within attention will\n",
    "        # complain.\n",
    "        encoder_outputs_mask = encoder_outputs_mask.float()\n",
    "        # (batch_size, input_sequence_length)\n",
    "        input_weights = self._decoder_attention(decoder_hidden_state, encoder_outputs, encoder_outputs_mask)\n",
    "        # (batch_size, encoder_output_dim)\n",
    "        attended_input = weighted_sum(encoder_outputs, input_weights)\n",
    "        # (batch_size, encoder_output_dim + target_embedding_dim)\n",
    "        return torch.cat((attended_input, embedded_input), -1)\n",
    "    \n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test\n",
    "        time, to finalize predictions. The logic for the decoder part of the encoder-decoder lives\n",
    "        within the ``forward`` method.\n",
    "        This method trims the output predictions to the first end symbol, replaces indices with\n",
    "        corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.\n",
    "        \"\"\"\n",
    "        predicted_indices = output_dict[\"predictions\"]\n",
    "        if not isinstance(predicted_indices, numpy.ndarray):\n",
    "            predicted_indices = predicted_indices.detach().cpu().numpy()\n",
    "        all_predicted_tokens = []\n",
    "        all_predicted_indices = []\n",
    "        for indices in predicted_indices:\n",
    "            indices = list(indices)\n",
    "            # Collect indices till the first end_symbol\n",
    "            if self._end_index in indices:\n",
    "                indices = indices[:indices.index(self._end_index)]\n",
    "            predicted_tokens = [self._target_vocab.get_token_from_index(x, namespace=\"tokens\")\n",
    "                                for x in indices]\n",
    "            all_predicted_tokens.append(predicted_tokens)\n",
    "            all_predicted_indices.append(indices)\n",
    "        output_dict[\"predicted_tokens\"] = all_predicted_tokens\n",
    "        \n",
    "        output_dict[\"predicted_indices\"] = all_predicted_indices  \n",
    "        \n",
    "        return output_dict\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _get_loss(logits: torch.LongTensor,\n",
    "#                   targets: torch.LongTensor,\n",
    "#                   target_mask: torch.LongTensor,\n",
    "#                   label_smoothing) -> torch.LongTensor:\n",
    "#         \"\"\"\n",
    "#         Takes logits (unnormalized outputs from the decoder) of size (batch_size,\n",
    "#         num_decoding_steps, num_classes), target indices of size (batch_size, num_decoding_steps+1)\n",
    "#         and corresponding masks of size (batch_size, num_decoding_steps+1) steps and computes cross\n",
    "#         entropy loss while taking the mask into account.\n",
    "#         The length of ``targets`` is expected to be greater than that of ``logits`` because the\n",
    "#         decoder does not need to compute the output corresponding to the last timestep of\n",
    "#         ``targets``. This method aligns the inputs appropriately to compute the loss.\n",
    "#         During training, we want the logit corresponding to timestep i to be similar to the target\n",
    "#         token from timestep i + 1. That is, the targets should be shifted by one timestep for\n",
    "#         appropriate comparison.  Consider a single example where the target has 3 words, and\n",
    "#         padding is to 7 tokens.\n",
    "#            The complete sequence would correspond to <S> w1  w2  w3  <E> <P> <P>\n",
    "#            and the mask would be                     1   1   1   1   1   0   0\n",
    "#            and let the logits be                     l1  l2  l3  l4  l5  l6\n",
    "#         We actually need to compare:\n",
    "#            the sequence           w1  w2  w3  <E> <P> <P>\n",
    "#            with masks             1   1   1   1   0   0\n",
    "#            against                l1  l2  l3  l4  l5  l6\n",
    "#            (where the input was)  <S> w1  w2  w3  <E> <P>\n",
    "#         \"\"\"\n",
    "#         relevant_targets = targets[:, 1:].contiguous()  # (batch_size, num_decoding_steps)\n",
    "#         relevant_mask = target_mask[:, 1:].contiguous()  # (batch_size, num_decoding_steps)\n",
    "#         loss = sequence_cross_entropy_with_logits(logits, relevant_targets, relevant_mask,                                                           label_smoothing = label_smoothing)\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_emb_params = Params({\"vocab_namespace\": \"tokens\",\n",
    "                      \"embedding_dim\": 300,\n",
    "                      \"pretrained_file\": None,\n",
    "                      \"trainable\": True\n",
    "                      })\n",
    "\n",
    "et_emb_params =  en_emb_params.duplicate()\n",
    "\n",
    "en_embedding = Embedding.from_params(vocab=en_vocab, params=en_emb_params)\n",
    "et_embedding = Embedding.from_params(vocab=et_vocab, params=et_emb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_lstm_params_en = Params({\n",
    "    \"type\": \"lstm\",\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"input_size\": 300,\n",
    "    \"hidden_size\": 600\n",
    "})\n",
    "\n",
    "seq2seq_lstm_params_et = seq2seq_lstm_params_en.duplicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "et2en_model = VanillaRnn2Rnn(source_vocab = et_vocab,\n",
    "                            target_vocab = en_vocab,\n",
    "                            source_embedding = et_embedding,\n",
    "                            target_embedding=en_embedding,\n",
    "                            encoder=Seq2SeqEncoder.from_params(params=seq2seq_lstm_params_et),\n",
    "                            max_decoding_steps=50)\n",
    "\n",
    "\n",
    "en2et_model = VanillaRnn2Rnn(source_vocab = en_vocab,\n",
    "                            target_vocab = et_vocab,\n",
    "                            source_embedding = en_embedding,\n",
    "                            target_embedding=et_embedding,\n",
    "                            encoder=Seq2SeqEncoder.from_params(params=seq2seq_lstm_params_en),\n",
    "                            max_decoding_steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en2et_model.decode(en2et_model.forward(**batch1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import Seq2VecEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Binary(Model):\n",
    "    \"\"\"\n",
    "    Logistic regression on sentence.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                vocab: Vocabulary,\n",
    "                embedding: Embedding,\n",
    "                seq2vec_encoder: Seq2VecEncoder):\n",
    "    \n",
    "        super(Seq2Binary, self).__init__(vocab)\n",
    "        \n",
    "        self._embedding = embedding\n",
    "        self._encoder = seq2vec_encoder\n",
    "        self._projection_layer = Linear(self._encoder.get_output_dim(), 1)\n",
    "        \n",
    "        \n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                sentence: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Decoder logic for producing the entire target sequence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence : torch.LongTensor\n",
    "           Tensor of padded batch of indexed source strings\n",
    "        \"\"\"\n",
    "        # (batch_size, input_sequence_length, encoder_output_dim)\n",
    "        #print(sentence)\n",
    "        tokens_ids = sentence[\"tokens\"]\n",
    "        embedded_input = self._embedding(tokens_ids)\n",
    "        batch_size, _, _ = embedded_input.size()\n",
    "        source_mask = get_text_field_mask(sentence)\n",
    "        final_encoder_output = self._encoder(embedded_input, source_mask)\n",
    "        logits = self._projection_layer(final_encoder_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        return {\"probs\": probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params_en = Params({\n",
    "                        \"type\": \"lstm\",\n",
    "                        \"bidirectional\": True,\n",
    "                        \"num_layers\": 1,\n",
    "                        \"input_size\": 300,\n",
    "                        \"hidden_size\": 600\n",
    "                        })\n",
    "\n",
    "classifier_params_et = classifier_params_en.duplicate()\n",
    "\n",
    "en_classifier = Seq2Binary(vocab=en_vocab, embedding=en_embedding, seq2vec_encoder=Seq2VecEncoder.from_params(classifier_params_en))\n",
    "et_classifier = Seq2Binary(vocab=et_vocab, embedding=et_embedding, seq2vec_encoder=Seq2VecEncoder.from_params(classifier_params_et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#et_classifier(**batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fake_batch(predicted_ids):\n",
    "    # TODO: ADD START END TOKENS\n",
    "    max_length = len(sorted(predicted_ids ,key=len, reverse=True)[0])\n",
    "    masked_ids = numpy.array([xi + [0] * (length - len(xi)) for xi in predicted_ids])\n",
    "    masked_ids_tensor = torch.from_numpy(masked_ids)\n",
    "    # MOVE TO GPU OPTINALLY\n",
    "    if torch.cuda.is_available():\n",
    "        masked_ids_tensor = masked_ids_tensor.cuda()\n",
    "        \n",
    "    return {'sentence': {'tokens': masked_ids_tensor}}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_et = en2et_model.decode(en2et_model.forward(**batch1))[\"predicted_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_et = prepare_fake_batch(fake_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': {'tokens': tensor([[11733, 11733, 13218,  ..., 13218, 13218, 13218],\n",
       "          [11733, 13218, 13218,  ..., 13218, 13218, 13218],\n",
       "          [11733, 11733, 13218,  ..., 13218, 13218, 13218],\n",
       "          ...,\n",
       "          [11733, 11733, 13218,  ..., 13218, 13218, 13218],\n",
       "          [11733, 11733, 13218,  ..., 13218, 13218, 13218],\n",
       "          [11733, 11733, 13218,  ..., 13218, 13218, 13218]])}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075],\n",
       "        [0.5075]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_classifier(**fake_et)[\"probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch1[\"sentence\"][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:konstrukt]",
   "language": "python",
   "name": "conda-env-konstrukt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
