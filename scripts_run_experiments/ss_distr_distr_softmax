#!/bin/bash

# DO NOT FORGET TO DO """chmod 755 slurm_specs.sbatch""" before

#The name of the job is test_job
#SBATCH -J diff_nmt

#The job requires 1 compute node
#SBATCH -N 1

#The job requires 1 task per node
#SBATCH --ntasks-per-node=1

#The maximum walltime of the job is a 5 hours
#SBATCH -t 05:00:00

#SBATCH --mem=7G

#Leave this here if you need a GPU for your job
#SBATCH --partition=gpu

#SBATCH --gres=gpu:tesla:1

#SBATCH --exclude=falcon3

#CHANGE THIS 1
#SBATCH --output=slurm_logs/ss_distr_distr_softmax.log

# OUR COMMANDS GO HERE

module load python/3.6.3/CUDA-8.0

source activate konstrukt-cuda8

#CHANGE THIS 2
EXP_NAME=ss_distr_distr_softmax
FOLDER_NAME=models/$EXP_NAME

rm -r $FOLDER_NAME

allennlp train --include-package my_library experiments/main.json -s $FOLDER_NAME \
-o \
 "{"model": \
        {"scheduled_sampling_ratio": 0.5}\

        {"self_feed_with": 'distribution'}\
        {"infer_with": 'distribution'}\

        {"weight_function": 'softmax'}\

        # has an effect only when weight function is 'gumbel'
        {"gumbel_tau": '1'}\
        {"gumbel_hard": 'true'}\
        {"gumbel_eps": '1e-10'}\

}"

# translate dev
allennlp predict $FOLDER_NAME/model.tar.gz data/de-en/valid.tok.do50.de --include-package my_library \
--predictor seq2seq --output-file hyps/$EXP_NAME.dev.en.hyps --cuda-device 0 --batch-size 100

# translate test
allennlp predict $FOLDER_NAME/model.tar.gz data/de-en/test.tok.do50.de --include-package my_library \
--predictor seq2seq --output-file hyps/$EXP_NAME.test.en.hyps --cuda-device 0 --batch-size 100
